# The zk-hs Service creates a domain for all of the Pods:
#
# zk-0.zk-hs.default.svc.cluster.local
# zk-1.zk-hs.default.svc.cluster.local
# zk-2.zk-hs.default.svc.cluster.local
apiVersion: v1
kind: Service
metadata:
  name: zk-hs
  labels:
    app: zk
spec:
  ports:
  # followers use to connect to the leader,
  - port: 2888
    name: server
  #  and the second is for leader election
  - port: 3888
    name: leader-election
  # headless - the DNS server will return the pod IPs instead of the single service IP.
  clusterIP: None
  selector:
    app: zk
---
apiVersion: v1
kind: Service
metadata:
  name: zk-cs
  labels:
    app: zk
spec:
  ports:
  # the port to listen for client connections; that is, the port that clients attempt to connect to
  - port: 2181
    name: client
  selector:
    app: zk
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
# limits the number pods of a replicated application that are down 
# simultaneously from voluntary disruptions.
metadata:
  name: zk-pdb
spec:
  selector:
    matchLabels:
      app: zk
  # which is a description of the number of pods from 
  # that set that can be unavailable after the eviction. 
  maxUnavailable: 1
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zk
spec:
  selector:
    matchLabels:
      app: zk
  serviceName: zk-hs
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  # allows you to relax its ordering guarantees while preserving its uniqueness and identity
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: zk
    spec:
      # allow you to constrain which nodes your pod is eligible to be scheduled 
      # based on labels on pods that are already running on the node rather than based on labels on nodes.
      #... “this pod should (or, in the case of anti-affinity, should not) run in an X if
      #  that X is already running one or more pods that meet rule Y”. Y is expressed as a LabelSelector 
      # .. X is a topology domain like node, rack, cloud provider zone, cloud provider region, etc. 
      # You express it using a topologyKey
      affinity:
        podAntiAffinity:
          # ... “co-locate the pods of service A and service B in the same zone, 
          # since they communicate a lot with each other”
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - zk
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: kubernetes-zookeeper
        imagePullPolicy: Always
        image: "k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10"
        resources:
          requests:
            memory: "254Mi"
            cpu: "0.2"
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        command:
        - sh
        - -c
        - "start-zookeeper \
          --servers=3 \
          --data_dir=/var/lib/zookeeper/data \
          --data_log_dir=/var/lib/zookeeper/data/log \
          --conf_dir=/opt/zookeeper/conf \
          --client_port=2181 \
          --election_port=3888 \
          --server_port=2888 \
          --tick_time=2000 \
          --init_limit=10 \
          --sync_limit=5 \
          --heap=64M \
          --max_client_cnxns=60 \
          --snap_retain_count=3 \
          --purge_interval=12 \
          --max_session_timeout=40000 \
          --min_session_timeout=4000 \
          --log_level=INFO"
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - "zookeeper-ready 2181"
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - "zookeeper-ready 2181"
          initialDelaySeconds: 10
          timeoutSeconds: 5
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/zookeeper
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 20Mi
